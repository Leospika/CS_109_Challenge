{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CS 109 Extension Project",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Part 1: Get All Necessary Data",
   "id": "a4becaa0cc618e9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T06:57:09.273671Z",
     "start_time": "2025-12-01T06:45:58.936113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Goal: Get Live Feed Data\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "# make a directory for the data\n",
    "download_directory = \"data\"\n",
    "# create the directory if it doesn't exist\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "# mark how many days in each month\n",
    "days = {2: 28, 4: 30, 6: 30, 9: 30, 11: 30}\n",
    "\n",
    "# loop through each day\n",
    "for i in range(1, 13):\n",
    "    day = 0\n",
    "    if i in days:\n",
    "        day = days[i]\n",
    "    else:\n",
    "        day = 31\n",
    "\n",
    "    for j in range(1, day + 1):\n",
    "        # get associated file name\n",
    "        file_name = f\"subwaydatanyc_2023-{i:02}-{j:02}_csv.tar.xz\"\n",
    "        full_path = os.path.join(download_directory, file_name)\n",
    "\n",
    "        # pull the data from the URL\n",
    "        url = f\"https://subwaydata.nyc/data/{file_name}\"\n",
    "\n",
    "        try:\n",
    "            # query for response\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # if successful, write data to laptop\n",
    "            if response.status_code == 200:\n",
    "                with open(full_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            # catch failures\n",
    "            else:\n",
    "                print(f\"Error {response.status_code} accessing: {file_name}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Connection Error for {file_name}: {e}\")\n",
    "            time.sleep(5) # Wait longer after a connection error"
   ],
   "id": "606e0b4a0526182e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leomelton/PyCharmMiscProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T08:45:52.972917Z",
     "start_time": "2025-12-01T08:45:00.200242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract from .tar.xz file extension\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# define the necessary file paths\n",
    "data_directory = \"data\"\n",
    "extract_dir = 'extracted_csv_data'\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, 13):\n",
    "    day = 0\n",
    "    if i in days:\n",
    "        day = days[i]\n",
    "    else:\n",
    "        day = 31\n",
    "\n",
    "    for j in range(1, day + 1):\n",
    "        # define files to grab\n",
    "        archive_file = os.path.join(data_directory, f'subwaydatanyc_2023-{i:02}-{j:02}_csv.tar.xz')\n",
    "        target_file_in_archive = f'subwaydatanyc_2023-{i:02}-{j:02}_stop_times.csv'\n",
    "\n",
    "        # open the archive, get just the stop_times.csv file\n",
    "        try:\n",
    "            print(f\"Starting extraction of ONLY {target_file_in_archive} from {archive_file}...\")\n",
    "\n",
    "            # read and decompress the file\n",
    "            with tarfile.open(archive_file, 'r:xz') as tar:\n",
    "\n",
    "                # check for file\n",
    "                if target_file_in_archive in tar.getnames():\n",
    "                    # extract to extracted_csv_data\n",
    "                    tar.extract(target_file_in_archive, path=extract_dir)\n",
    "\n",
    "                    # create full path\n",
    "                    extracted_path = os.path.join(extract_dir, target_file_in_archive)\n",
    "\n",
    "        # catch errors\n",
    "        except tarfile.ReadError:\n",
    "            print(f\"\\n❌ Error: Could not read or decompress the archive '{archive_file}'. Check file integrity.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ An unexpected error occurred: {e}\")"
   ],
   "id": "6945a3aa68321f67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting extraction of ONLY subwaydatanyc_2023-01-01_stop_times.csv from data/subwaydatanyc_2023-01-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-02_stop_times.csv from data/subwaydatanyc_2023-01-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-03_stop_times.csv from data/subwaydatanyc_2023-01-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-04_stop_times.csv from data/subwaydatanyc_2023-01-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-05_stop_times.csv from data/subwaydatanyc_2023-01-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-06_stop_times.csv from data/subwaydatanyc_2023-01-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-07_stop_times.csv from data/subwaydatanyc_2023-01-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-08_stop_times.csv from data/subwaydatanyc_2023-01-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-09_stop_times.csv from data/subwaydatanyc_2023-01-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-10_stop_times.csv from data/subwaydatanyc_2023-01-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-11_stop_times.csv from data/subwaydatanyc_2023-01-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-12_stop_times.csv from data/subwaydatanyc_2023-01-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-13_stop_times.csv from data/subwaydatanyc_2023-01-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-14_stop_times.csv from data/subwaydatanyc_2023-01-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-15_stop_times.csv from data/subwaydatanyc_2023-01-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-16_stop_times.csv from data/subwaydatanyc_2023-01-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-17_stop_times.csv from data/subwaydatanyc_2023-01-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-18_stop_times.csv from data/subwaydatanyc_2023-01-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-19_stop_times.csv from data/subwaydatanyc_2023-01-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-20_stop_times.csv from data/subwaydatanyc_2023-01-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-21_stop_times.csv from data/subwaydatanyc_2023-01-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-22_stop_times.csv from data/subwaydatanyc_2023-01-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-23_stop_times.csv from data/subwaydatanyc_2023-01-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-24_stop_times.csv from data/subwaydatanyc_2023-01-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-25_stop_times.csv from data/subwaydatanyc_2023-01-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-26_stop_times.csv from data/subwaydatanyc_2023-01-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-27_stop_times.csv from data/subwaydatanyc_2023-01-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-28_stop_times.csv from data/subwaydatanyc_2023-01-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-29_stop_times.csv from data/subwaydatanyc_2023-01-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-30_stop_times.csv from data/subwaydatanyc_2023-01-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-01-31_stop_times.csv from data/subwaydatanyc_2023-01-31_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-01_stop_times.csv from data/subwaydatanyc_2023-02-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-02_stop_times.csv from data/subwaydatanyc_2023-02-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-03_stop_times.csv from data/subwaydatanyc_2023-02-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-04_stop_times.csv from data/subwaydatanyc_2023-02-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-05_stop_times.csv from data/subwaydatanyc_2023-02-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-06_stop_times.csv from data/subwaydatanyc_2023-02-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-07_stop_times.csv from data/subwaydatanyc_2023-02-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-08_stop_times.csv from data/subwaydatanyc_2023-02-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-09_stop_times.csv from data/subwaydatanyc_2023-02-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-10_stop_times.csv from data/subwaydatanyc_2023-02-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-11_stop_times.csv from data/subwaydatanyc_2023-02-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-12_stop_times.csv from data/subwaydatanyc_2023-02-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-13_stop_times.csv from data/subwaydatanyc_2023-02-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-14_stop_times.csv from data/subwaydatanyc_2023-02-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-15_stop_times.csv from data/subwaydatanyc_2023-02-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-16_stop_times.csv from data/subwaydatanyc_2023-02-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-17_stop_times.csv from data/subwaydatanyc_2023-02-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-18_stop_times.csv from data/subwaydatanyc_2023-02-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-19_stop_times.csv from data/subwaydatanyc_2023-02-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-20_stop_times.csv from data/subwaydatanyc_2023-02-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-21_stop_times.csv from data/subwaydatanyc_2023-02-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-22_stop_times.csv from data/subwaydatanyc_2023-02-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-23_stop_times.csv from data/subwaydatanyc_2023-02-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-24_stop_times.csv from data/subwaydatanyc_2023-02-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-25_stop_times.csv from data/subwaydatanyc_2023-02-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-26_stop_times.csv from data/subwaydatanyc_2023-02-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-27_stop_times.csv from data/subwaydatanyc_2023-02-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-02-28_stop_times.csv from data/subwaydatanyc_2023-02-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-01_stop_times.csv from data/subwaydatanyc_2023-03-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-02_stop_times.csv from data/subwaydatanyc_2023-03-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-03_stop_times.csv from data/subwaydatanyc_2023-03-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-04_stop_times.csv from data/subwaydatanyc_2023-03-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-05_stop_times.csv from data/subwaydatanyc_2023-03-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-06_stop_times.csv from data/subwaydatanyc_2023-03-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-07_stop_times.csv from data/subwaydatanyc_2023-03-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-08_stop_times.csv from data/subwaydatanyc_2023-03-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-09_stop_times.csv from data/subwaydatanyc_2023-03-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-10_stop_times.csv from data/subwaydatanyc_2023-03-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-11_stop_times.csv from data/subwaydatanyc_2023-03-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-12_stop_times.csv from data/subwaydatanyc_2023-03-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-13_stop_times.csv from data/subwaydatanyc_2023-03-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-14_stop_times.csv from data/subwaydatanyc_2023-03-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-15_stop_times.csv from data/subwaydatanyc_2023-03-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-16_stop_times.csv from data/subwaydatanyc_2023-03-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-17_stop_times.csv from data/subwaydatanyc_2023-03-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-18_stop_times.csv from data/subwaydatanyc_2023-03-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-19_stop_times.csv from data/subwaydatanyc_2023-03-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-20_stop_times.csv from data/subwaydatanyc_2023-03-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-21_stop_times.csv from data/subwaydatanyc_2023-03-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-22_stop_times.csv from data/subwaydatanyc_2023-03-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-23_stop_times.csv from data/subwaydatanyc_2023-03-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-24_stop_times.csv from data/subwaydatanyc_2023-03-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-25_stop_times.csv from data/subwaydatanyc_2023-03-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-26_stop_times.csv from data/subwaydatanyc_2023-03-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-27_stop_times.csv from data/subwaydatanyc_2023-03-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-28_stop_times.csv from data/subwaydatanyc_2023-03-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-29_stop_times.csv from data/subwaydatanyc_2023-03-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-30_stop_times.csv from data/subwaydatanyc_2023-03-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-03-31_stop_times.csv from data/subwaydatanyc_2023-03-31_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-01_stop_times.csv from data/subwaydatanyc_2023-04-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-02_stop_times.csv from data/subwaydatanyc_2023-04-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-03_stop_times.csv from data/subwaydatanyc_2023-04-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-04_stop_times.csv from data/subwaydatanyc_2023-04-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-05_stop_times.csv from data/subwaydatanyc_2023-04-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-06_stop_times.csv from data/subwaydatanyc_2023-04-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-07_stop_times.csv from data/subwaydatanyc_2023-04-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-08_stop_times.csv from data/subwaydatanyc_2023-04-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-09_stop_times.csv from data/subwaydatanyc_2023-04-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-10_stop_times.csv from data/subwaydatanyc_2023-04-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-11_stop_times.csv from data/subwaydatanyc_2023-04-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-12_stop_times.csv from data/subwaydatanyc_2023-04-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-13_stop_times.csv from data/subwaydatanyc_2023-04-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-14_stop_times.csv from data/subwaydatanyc_2023-04-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-15_stop_times.csv from data/subwaydatanyc_2023-04-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-16_stop_times.csv from data/subwaydatanyc_2023-04-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-17_stop_times.csv from data/subwaydatanyc_2023-04-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-18_stop_times.csv from data/subwaydatanyc_2023-04-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-19_stop_times.csv from data/subwaydatanyc_2023-04-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-20_stop_times.csv from data/subwaydatanyc_2023-04-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-21_stop_times.csv from data/subwaydatanyc_2023-04-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-22_stop_times.csv from data/subwaydatanyc_2023-04-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-23_stop_times.csv from data/subwaydatanyc_2023-04-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-24_stop_times.csv from data/subwaydatanyc_2023-04-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-25_stop_times.csv from data/subwaydatanyc_2023-04-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-26_stop_times.csv from data/subwaydatanyc_2023-04-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-27_stop_times.csv from data/subwaydatanyc_2023-04-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-28_stop_times.csv from data/subwaydatanyc_2023-04-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-29_stop_times.csv from data/subwaydatanyc_2023-04-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-04-30_stop_times.csv from data/subwaydatanyc_2023-04-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-01_stop_times.csv from data/subwaydatanyc_2023-05-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-02_stop_times.csv from data/subwaydatanyc_2023-05-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-03_stop_times.csv from data/subwaydatanyc_2023-05-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-04_stop_times.csv from data/subwaydatanyc_2023-05-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-05_stop_times.csv from data/subwaydatanyc_2023-05-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-06_stop_times.csv from data/subwaydatanyc_2023-05-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-07_stop_times.csv from data/subwaydatanyc_2023-05-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-08_stop_times.csv from data/subwaydatanyc_2023-05-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-09_stop_times.csv from data/subwaydatanyc_2023-05-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-10_stop_times.csv from data/subwaydatanyc_2023-05-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-11_stop_times.csv from data/subwaydatanyc_2023-05-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-12_stop_times.csv from data/subwaydatanyc_2023-05-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-13_stop_times.csv from data/subwaydatanyc_2023-05-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-14_stop_times.csv from data/subwaydatanyc_2023-05-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-15_stop_times.csv from data/subwaydatanyc_2023-05-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-16_stop_times.csv from data/subwaydatanyc_2023-05-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-17_stop_times.csv from data/subwaydatanyc_2023-05-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-18_stop_times.csv from data/subwaydatanyc_2023-05-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-19_stop_times.csv from data/subwaydatanyc_2023-05-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-20_stop_times.csv from data/subwaydatanyc_2023-05-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-21_stop_times.csv from data/subwaydatanyc_2023-05-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-22_stop_times.csv from data/subwaydatanyc_2023-05-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-23_stop_times.csv from data/subwaydatanyc_2023-05-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-24_stop_times.csv from data/subwaydatanyc_2023-05-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-25_stop_times.csv from data/subwaydatanyc_2023-05-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-26_stop_times.csv from data/subwaydatanyc_2023-05-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-27_stop_times.csv from data/subwaydatanyc_2023-05-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-28_stop_times.csv from data/subwaydatanyc_2023-05-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-29_stop_times.csv from data/subwaydatanyc_2023-05-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-30_stop_times.csv from data/subwaydatanyc_2023-05-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-05-31_stop_times.csv from data/subwaydatanyc_2023-05-31_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-01_stop_times.csv from data/subwaydatanyc_2023-06-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-02_stop_times.csv from data/subwaydatanyc_2023-06-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-03_stop_times.csv from data/subwaydatanyc_2023-06-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-04_stop_times.csv from data/subwaydatanyc_2023-06-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-05_stop_times.csv from data/subwaydatanyc_2023-06-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-06_stop_times.csv from data/subwaydatanyc_2023-06-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-07_stop_times.csv from data/subwaydatanyc_2023-06-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-08_stop_times.csv from data/subwaydatanyc_2023-06-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-09_stop_times.csv from data/subwaydatanyc_2023-06-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-10_stop_times.csv from data/subwaydatanyc_2023-06-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-11_stop_times.csv from data/subwaydatanyc_2023-06-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-12_stop_times.csv from data/subwaydatanyc_2023-06-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-13_stop_times.csv from data/subwaydatanyc_2023-06-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-14_stop_times.csv from data/subwaydatanyc_2023-06-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-15_stop_times.csv from data/subwaydatanyc_2023-06-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-16_stop_times.csv from data/subwaydatanyc_2023-06-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-17_stop_times.csv from data/subwaydatanyc_2023-06-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-18_stop_times.csv from data/subwaydatanyc_2023-06-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-19_stop_times.csv from data/subwaydatanyc_2023-06-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-20_stop_times.csv from data/subwaydatanyc_2023-06-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-21_stop_times.csv from data/subwaydatanyc_2023-06-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-22_stop_times.csv from data/subwaydatanyc_2023-06-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-23_stop_times.csv from data/subwaydatanyc_2023-06-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-24_stop_times.csv from data/subwaydatanyc_2023-06-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-25_stop_times.csv from data/subwaydatanyc_2023-06-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-26_stop_times.csv from data/subwaydatanyc_2023-06-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-27_stop_times.csv from data/subwaydatanyc_2023-06-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-28_stop_times.csv from data/subwaydatanyc_2023-06-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-29_stop_times.csv from data/subwaydatanyc_2023-06-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-06-30_stop_times.csv from data/subwaydatanyc_2023-06-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-01_stop_times.csv from data/subwaydatanyc_2023-07-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-02_stop_times.csv from data/subwaydatanyc_2023-07-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-03_stop_times.csv from data/subwaydatanyc_2023-07-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-04_stop_times.csv from data/subwaydatanyc_2023-07-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-05_stop_times.csv from data/subwaydatanyc_2023-07-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-06_stop_times.csv from data/subwaydatanyc_2023-07-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-07_stop_times.csv from data/subwaydatanyc_2023-07-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-08_stop_times.csv from data/subwaydatanyc_2023-07-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-09_stop_times.csv from data/subwaydatanyc_2023-07-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-10_stop_times.csv from data/subwaydatanyc_2023-07-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-11_stop_times.csv from data/subwaydatanyc_2023-07-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-12_stop_times.csv from data/subwaydatanyc_2023-07-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-13_stop_times.csv from data/subwaydatanyc_2023-07-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-14_stop_times.csv from data/subwaydatanyc_2023-07-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-15_stop_times.csv from data/subwaydatanyc_2023-07-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-16_stop_times.csv from data/subwaydatanyc_2023-07-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-17_stop_times.csv from data/subwaydatanyc_2023-07-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-18_stop_times.csv from data/subwaydatanyc_2023-07-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-19_stop_times.csv from data/subwaydatanyc_2023-07-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-20_stop_times.csv from data/subwaydatanyc_2023-07-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-21_stop_times.csv from data/subwaydatanyc_2023-07-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-22_stop_times.csv from data/subwaydatanyc_2023-07-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-23_stop_times.csv from data/subwaydatanyc_2023-07-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-24_stop_times.csv from data/subwaydatanyc_2023-07-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-25_stop_times.csv from data/subwaydatanyc_2023-07-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-26_stop_times.csv from data/subwaydatanyc_2023-07-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-27_stop_times.csv from data/subwaydatanyc_2023-07-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-28_stop_times.csv from data/subwaydatanyc_2023-07-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-29_stop_times.csv from data/subwaydatanyc_2023-07-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-30_stop_times.csv from data/subwaydatanyc_2023-07-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-07-31_stop_times.csv from data/subwaydatanyc_2023-07-31_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-01_stop_times.csv from data/subwaydatanyc_2023-08-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-02_stop_times.csv from data/subwaydatanyc_2023-08-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-03_stop_times.csv from data/subwaydatanyc_2023-08-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-04_stop_times.csv from data/subwaydatanyc_2023-08-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-05_stop_times.csv from data/subwaydatanyc_2023-08-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-06_stop_times.csv from data/subwaydatanyc_2023-08-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-07_stop_times.csv from data/subwaydatanyc_2023-08-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-08_stop_times.csv from data/subwaydatanyc_2023-08-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-09_stop_times.csv from data/subwaydatanyc_2023-08-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-10_stop_times.csv from data/subwaydatanyc_2023-08-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-11_stop_times.csv from data/subwaydatanyc_2023-08-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-12_stop_times.csv from data/subwaydatanyc_2023-08-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-13_stop_times.csv from data/subwaydatanyc_2023-08-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-14_stop_times.csv from data/subwaydatanyc_2023-08-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-15_stop_times.csv from data/subwaydatanyc_2023-08-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-16_stop_times.csv from data/subwaydatanyc_2023-08-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-17_stop_times.csv from data/subwaydatanyc_2023-08-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-18_stop_times.csv from data/subwaydatanyc_2023-08-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-19_stop_times.csv from data/subwaydatanyc_2023-08-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-20_stop_times.csv from data/subwaydatanyc_2023-08-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-21_stop_times.csv from data/subwaydatanyc_2023-08-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-22_stop_times.csv from data/subwaydatanyc_2023-08-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-23_stop_times.csv from data/subwaydatanyc_2023-08-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-24_stop_times.csv from data/subwaydatanyc_2023-08-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-25_stop_times.csv from data/subwaydatanyc_2023-08-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-26_stop_times.csv from data/subwaydatanyc_2023-08-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-27_stop_times.csv from data/subwaydatanyc_2023-08-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-28_stop_times.csv from data/subwaydatanyc_2023-08-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-29_stop_times.csv from data/subwaydatanyc_2023-08-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-30_stop_times.csv from data/subwaydatanyc_2023-08-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-08-31_stop_times.csv from data/subwaydatanyc_2023-08-31_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-01_stop_times.csv from data/subwaydatanyc_2023-09-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-02_stop_times.csv from data/subwaydatanyc_2023-09-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-03_stop_times.csv from data/subwaydatanyc_2023-09-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-04_stop_times.csv from data/subwaydatanyc_2023-09-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-05_stop_times.csv from data/subwaydatanyc_2023-09-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-06_stop_times.csv from data/subwaydatanyc_2023-09-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-07_stop_times.csv from data/subwaydatanyc_2023-09-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-08_stop_times.csv from data/subwaydatanyc_2023-09-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-09_stop_times.csv from data/subwaydatanyc_2023-09-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-10_stop_times.csv from data/subwaydatanyc_2023-09-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-11_stop_times.csv from data/subwaydatanyc_2023-09-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-12_stop_times.csv from data/subwaydatanyc_2023-09-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-13_stop_times.csv from data/subwaydatanyc_2023-09-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-14_stop_times.csv from data/subwaydatanyc_2023-09-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-15_stop_times.csv from data/subwaydatanyc_2023-09-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-16_stop_times.csv from data/subwaydatanyc_2023-09-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-17_stop_times.csv from data/subwaydatanyc_2023-09-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-18_stop_times.csv from data/subwaydatanyc_2023-09-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-19_stop_times.csv from data/subwaydatanyc_2023-09-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-20_stop_times.csv from data/subwaydatanyc_2023-09-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-21_stop_times.csv from data/subwaydatanyc_2023-09-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-22_stop_times.csv from data/subwaydatanyc_2023-09-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-23_stop_times.csv from data/subwaydatanyc_2023-09-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-24_stop_times.csv from data/subwaydatanyc_2023-09-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-25_stop_times.csv from data/subwaydatanyc_2023-09-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-26_stop_times.csv from data/subwaydatanyc_2023-09-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-27_stop_times.csv from data/subwaydatanyc_2023-09-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-28_stop_times.csv from data/subwaydatanyc_2023-09-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-29_stop_times.csv from data/subwaydatanyc_2023-09-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-09-30_stop_times.csv from data/subwaydatanyc_2023-09-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-01_stop_times.csv from data/subwaydatanyc_2023-10-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-02_stop_times.csv from data/subwaydatanyc_2023-10-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-03_stop_times.csv from data/subwaydatanyc_2023-10-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-04_stop_times.csv from data/subwaydatanyc_2023-10-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-05_stop_times.csv from data/subwaydatanyc_2023-10-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-06_stop_times.csv from data/subwaydatanyc_2023-10-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-07_stop_times.csv from data/subwaydatanyc_2023-10-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-08_stop_times.csv from data/subwaydatanyc_2023-10-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-09_stop_times.csv from data/subwaydatanyc_2023-10-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-10_stop_times.csv from data/subwaydatanyc_2023-10-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-11_stop_times.csv from data/subwaydatanyc_2023-10-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-12_stop_times.csv from data/subwaydatanyc_2023-10-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-13_stop_times.csv from data/subwaydatanyc_2023-10-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-14_stop_times.csv from data/subwaydatanyc_2023-10-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-15_stop_times.csv from data/subwaydatanyc_2023-10-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-16_stop_times.csv from data/subwaydatanyc_2023-10-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-17_stop_times.csv from data/subwaydatanyc_2023-10-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-18_stop_times.csv from data/subwaydatanyc_2023-10-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-19_stop_times.csv from data/subwaydatanyc_2023-10-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-20_stop_times.csv from data/subwaydatanyc_2023-10-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-21_stop_times.csv from data/subwaydatanyc_2023-10-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-22_stop_times.csv from data/subwaydatanyc_2023-10-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-23_stop_times.csv from data/subwaydatanyc_2023-10-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-24_stop_times.csv from data/subwaydatanyc_2023-10-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-25_stop_times.csv from data/subwaydatanyc_2023-10-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-26_stop_times.csv from data/subwaydatanyc_2023-10-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-27_stop_times.csv from data/subwaydatanyc_2023-10-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-28_stop_times.csv from data/subwaydatanyc_2023-10-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-29_stop_times.csv from data/subwaydatanyc_2023-10-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-30_stop_times.csv from data/subwaydatanyc_2023-10-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-10-31_stop_times.csv from data/subwaydatanyc_2023-10-31_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-01_stop_times.csv from data/subwaydatanyc_2023-11-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-02_stop_times.csv from data/subwaydatanyc_2023-11-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-03_stop_times.csv from data/subwaydatanyc_2023-11-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-04_stop_times.csv from data/subwaydatanyc_2023-11-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-05_stop_times.csv from data/subwaydatanyc_2023-11-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-06_stop_times.csv from data/subwaydatanyc_2023-11-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-07_stop_times.csv from data/subwaydatanyc_2023-11-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-08_stop_times.csv from data/subwaydatanyc_2023-11-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-09_stop_times.csv from data/subwaydatanyc_2023-11-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-10_stop_times.csv from data/subwaydatanyc_2023-11-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-11_stop_times.csv from data/subwaydatanyc_2023-11-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-12_stop_times.csv from data/subwaydatanyc_2023-11-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-13_stop_times.csv from data/subwaydatanyc_2023-11-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-14_stop_times.csv from data/subwaydatanyc_2023-11-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-15_stop_times.csv from data/subwaydatanyc_2023-11-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-16_stop_times.csv from data/subwaydatanyc_2023-11-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-17_stop_times.csv from data/subwaydatanyc_2023-11-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-18_stop_times.csv from data/subwaydatanyc_2023-11-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-19_stop_times.csv from data/subwaydatanyc_2023-11-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-20_stop_times.csv from data/subwaydatanyc_2023-11-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-21_stop_times.csv from data/subwaydatanyc_2023-11-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-22_stop_times.csv from data/subwaydatanyc_2023-11-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-23_stop_times.csv from data/subwaydatanyc_2023-11-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-24_stop_times.csv from data/subwaydatanyc_2023-11-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-25_stop_times.csv from data/subwaydatanyc_2023-11-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-26_stop_times.csv from data/subwaydatanyc_2023-11-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-27_stop_times.csv from data/subwaydatanyc_2023-11-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-28_stop_times.csv from data/subwaydatanyc_2023-11-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-29_stop_times.csv from data/subwaydatanyc_2023-11-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-11-30_stop_times.csv from data/subwaydatanyc_2023-11-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-01_stop_times.csv from data/subwaydatanyc_2023-12-01_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-02_stop_times.csv from data/subwaydatanyc_2023-12-02_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-03_stop_times.csv from data/subwaydatanyc_2023-12-03_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-04_stop_times.csv from data/subwaydatanyc_2023-12-04_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-05_stop_times.csv from data/subwaydatanyc_2023-12-05_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-06_stop_times.csv from data/subwaydatanyc_2023-12-06_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-07_stop_times.csv from data/subwaydatanyc_2023-12-07_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-08_stop_times.csv from data/subwaydatanyc_2023-12-08_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-09_stop_times.csv from data/subwaydatanyc_2023-12-09_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-10_stop_times.csv from data/subwaydatanyc_2023-12-10_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-11_stop_times.csv from data/subwaydatanyc_2023-12-11_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-12_stop_times.csv from data/subwaydatanyc_2023-12-12_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-13_stop_times.csv from data/subwaydatanyc_2023-12-13_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-14_stop_times.csv from data/subwaydatanyc_2023-12-14_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-15_stop_times.csv from data/subwaydatanyc_2023-12-15_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-16_stop_times.csv from data/subwaydatanyc_2023-12-16_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-17_stop_times.csv from data/subwaydatanyc_2023-12-17_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-18_stop_times.csv from data/subwaydatanyc_2023-12-18_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-19_stop_times.csv from data/subwaydatanyc_2023-12-19_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-20_stop_times.csv from data/subwaydatanyc_2023-12-20_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-21_stop_times.csv from data/subwaydatanyc_2023-12-21_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-22_stop_times.csv from data/subwaydatanyc_2023-12-22_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-23_stop_times.csv from data/subwaydatanyc_2023-12-23_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-24_stop_times.csv from data/subwaydatanyc_2023-12-24_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-25_stop_times.csv from data/subwaydatanyc_2023-12-25_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-26_stop_times.csv from data/subwaydatanyc_2023-12-26_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-27_stop_times.csv from data/subwaydatanyc_2023-12-27_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-28_stop_times.csv from data/subwaydatanyc_2023-12-28_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-29_stop_times.csv from data/subwaydatanyc_2023-12-29_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-30_stop_times.csv from data/subwaydatanyc_2023-12-30_csv.tar.xz...\n",
      "Starting extraction of ONLY subwaydatanyc_2023-12-31_stop_times.csv from data/subwaydatanyc_2023-12-31_csv.tar.xz...\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T04:14:25.477971Z",
     "start_time": "2025-12-03T04:12:35.743816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gather Data from CSV Files\n",
    "import pandas as pd\n",
    "\n",
    "days = {2: 28, 4: 30, 6: 30, 9: 30, 11: 30}\n",
    "\n",
    "# Function to classify a piece of data as \"Rush Hour\", \"Weekends\", or \"Nights\" for use for hour slicing\n",
    "# given a timestamp.\n",
    "def categorize_time(timestamp):\n",
    "\n",
    "    # get a day of the week\n",
    "    day = timestamp.weekday()\n",
    "    hour = timestamp.hour\n",
    "\n",
    "    # Nights (10 p.m. to 6 a.m. EVERY DAY)\n",
    "    # window split into two chunks: (10 PM - 11:59 PM) OR (12 AM - 5:59 AM)\n",
    "    is_night = (hour >= 22) or (hour < 6)\n",
    "    if is_night:\n",
    "        return 'Nights'\n",
    "\n",
    "    # Weekends (Saturday=5, Sunday=6 in datetime)\n",
    "    is_weekend = (day >= 5)\n",
    "    if is_weekend:\n",
    "        return 'Weekends'\n",
    "\n",
    "    # Rush Hour (6 a.m. to 10 a.m. OR 4 p.m. to 8 p.m. on Weekdays)\n",
    "    is_am_rush = (hour >= 6) and (hour < 10)\n",
    "    is_pm_rush = (hour >= 16) and (hour < 20)\n",
    "\n",
    "    # AM or PM rush is \"Rush Hour\" classification\n",
    "    if is_am_rush or is_pm_rush:\n",
    "        return 'Rush Hour'\n",
    "\n",
    "    # Last category, not used in analysis\n",
    "    return 'Off-Peak Weekday'\n",
    "\n",
    "# Create dicts to hold the three categories for F train and generally\n",
    "f_delays_by_category = {'Rush Hour': [], 'Weekends': [], 'Nights': []}\n",
    "all_delays_by_category = {'Rush Hour': [], 'Weekends': [], 'Nights': []}\n",
    "\n",
    "# Arrays to hold: all delays, delays on the F train, and delays on the F train with an additional slice\n",
    "# for stop name\n",
    "all_delay = []\n",
    "f_delay = []\n",
    "f_delay_by_stop = []\n",
    "\n",
    "# again, loop by day (each day has one CSV file)\n",
    "for i in range(1, 13):\n",
    "    day = 0\n",
    "    if i in days:\n",
    "        day = days[i]\n",
    "    else:\n",
    "        day = 31\n",
    "\n",
    "    for j in range(1, day + 1):\n",
    "        # read in CSV\n",
    "        df = pd.read_csv(f\"extracted_csv_data/subwaydatanyc_2023-{i:02}-{j:02}_stop_times.csv\")\n",
    "\n",
    "        # get next departure\n",
    "        df['next_actual_departure'] = df.groupby('trip_uid')['marked_past'].shift(-1)\n",
    "\n",
    "        # calculate trip duration between stops\n",
    "        df['actual_duration_seconds'] = (\n",
    "            df['next_actual_departure'] - df['marked_past']\n",
    "        )\n",
    "\n",
    "        # get next expected departure\n",
    "        df['next_scheduled_departure'] = df['departure_time'].shift(-1)\n",
    "\n",
    "        # get expected length of trip\n",
    "        df['expected_duration_seconds'] = (\n",
    "            df['next_scheduled_departure'] - df['departure_time']\n",
    "        )\n",
    "\n",
    "        # filter out trips that are too short (likely erroneous data)\n",
    "        df_filtered = df[df['actual_duration_seconds'] > (df['expected_duration_seconds'] * 0.5)]\n",
    "        df_filtered = df_filtered[df_filtered['actual_duration_seconds'] > 30]\n",
    "\n",
    "        # calculate delay added\n",
    "        df_filtered['delay_added_seconds'] = (\n",
    "            df_filtered['actual_duration_seconds'] - df_filtered['expected_duration_seconds']\n",
    "        )\n",
    "\n",
    "        # remove faulty NaN data\n",
    "        df_final = df_filtered.dropna(subset=['delay_added_seconds'])\n",
    "\n",
    "        # Data filtering to remove some anomalies in the data set\n",
    "        # take out data of trains that start really late\n",
    "        df_internal_run = (\n",
    "            df_final.groupby('trip_uid')\n",
    "            .tail(-1)\n",
    "        )\n",
    "        # toss all data over 10800 seconds (3 hour delay), too egregious to include in data\n",
    "        MAX_DELAY_SECONDS = 10800\n",
    "        df_internal_run = df_internal_run[\n",
    "            df_internal_run['delay_added_seconds'] <= MAX_DELAY_SECONDS\n",
    "        ].copy()\n",
    "        # take out stops in Staten Island that are not being considered\n",
    "        df_subway_only = df_internal_run[~df_internal_run['stop_id'].str.startswith('S')].copy()\n",
    "\n",
    "        # Take the CSV data and add it to respective files\n",
    "\n",
    "        # Filter for F trains and put it into its own df\n",
    "        df_f_train = df_subway_only[\n",
    "            df_subway_only['trip_uid'].astype(str).str.contains(r'_F\\.\\.', regex=True)\n",
    "        ].copy()\n",
    "\n",
    "        # place into respective arrays\n",
    "        all_delay.append(df_subway_only['delay_added_seconds']) # all train delays\n",
    "        f_delay.append(df_f_train['delay_added_seconds']) # F train delay\n",
    "        f_delay_by_stop.append(df_f_train[['stop_id', 'delay_added_seconds']]) # F train, including stop slice\n",
    "\n",
    "        # convert Unix time from CSV files to datetime\n",
    "        df_f_train['datetime_local'] = pd.to_datetime(\n",
    "            df_f_train['marked_past'],\n",
    "            unit='s',\n",
    "            errors='coerce'\n",
    "        )\n",
    "        df_subway_only['datetime_local'] = pd.to_datetime(\n",
    "            df_subway_only['marked_past'],\n",
    "            unit='s',\n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "        # again, drop rows where conversion fails\n",
    "        df_f_train.dropna(subset=['datetime_local'], inplace=True)\n",
    "        df_subway_only.dropna(subset=['datetime_local'], inplace=True)\n",
    "\n",
    "\n",
    "        # apply categorization function to data\n",
    "        df_f_train['time_category'] = df_f_train['datetime_local'].apply(categorize_time)\n",
    "        df_subway_only['time_category'] = df_subway_only['datetime_local'].apply(categorize_time)\n",
    "\n",
    "        # slice by category\n",
    "        for category in ['Rush Hour', 'Weekends', 'Nights']:\n",
    "\n",
    "            # add F-train data\n",
    "            df_f_cat = df_f_train[df_f_train['time_category'] == category]\n",
    "            if not df_f_cat.empty:\n",
    "                f_delays_by_category[category].append(df_f_cat['delay_added_seconds'])\n",
    "\n",
    "            # add system data\n",
    "            df_all_cat = df_subway_only[df_subway_only['time_category'] == category]\n",
    "            if not df_all_cat.empty:\n",
    "                all_delays_by_category[category].append(df_all_cat['delay_added_seconds'])\n",
    "\n",
    "# Previous Experimental Code to Gather per-Route Info\n",
    "#         total_average = df_subway_only['delay_added_seconds'].mean()\n",
    "#         route_data['total'].append(total_average)\n",
    "#\n",
    "#         for route in trains:\n",
    "#             route_df = df_subway_only[\n",
    "#                 df_subway_only['trip_uid'].str.contains(fr'_{route}\\.\\.', regex=True)\n",
    "#             ].copy()\n",
    "#\n",
    "#             route_avg = route_df['delay_added_seconds'].mean()\n",
    "#             route_data[route].append(route_avg)\n",
    "#\n",
    "# for route in route_data:\n",
    "#     average = sum(route_data[route]) / len(route_data[route])\n",
    "#     print(\"Average of \" + route + \" is \" + str(average))"
   ],
   "id": "ca61c9ad5d6f0f32",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initial Analysis: Overall Dataset",
   "id": "86e0c8b35775af7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T04:12:10.006684Z",
     "start_time": "2025-12-03T04:12:09.988458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Construct PMF\n",
    "# concatenate data together\n",
    "all_delays_series = pd.concat(all_delay, ignore_index=True)\n",
    "f_delays_series = pd.concat(f_delay, ignore_index=True)\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "# chop-off extremes (more than 5 minutes early and more than 20 minutes late) and\n",
    "# mark them as -300 sec and 1200 sec respectively to go into buckets\n",
    "modified_f_del = [1200 if x > 1200 else -300 if x < -300 else x for x in f_delays_series]\n",
    "modified_all_del = [1200 if x > 1200 else -300 if x < -300 else x for x in all_delays_series]\n",
    "\n",
    "# Construct bins for histogram\n",
    "min_delay = -300\n",
    "max_delay = 1200\n",
    "bin_width = 60\n",
    "bins = np.arange(\n",
    "    start=np.floor(min_delay / bin_width) * bin_width,\n",
    "    stop=np.ceil(max_delay / bin_width) * bin_width + bin_width,\n",
    "    step=bin_width\n",
    ")\n",
    "print(bins)\n",
    "\n",
    "# Construct histogram using data\n",
    "hist_total, _ = np.histogram(modified_all_del, bins=bins, density=True)\n",
    "hist_f, _ = np.histogram(modified_f_del, bins=bins, density=True)\n"
   ],
   "id": "d32863ed4f4b31f2",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_delay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Construct PMF\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# concatenate data together\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m all_delays_series \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(\u001B[43mall_delay\u001B[49m, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m f_delays_series \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(f_delay, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# imports\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'all_delay' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:01:16.330003Z",
     "start_time": "2025-12-02T05:01:16.174542Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 39,
   "source": [
    "# Plot the Initial Distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate center of bins\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Calculate width of bars\n",
    "bar_width = bins[1] - bins[0]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the F Train PMF\n",
    "plt.bar(\n",
    "    bin_centers,\n",
    "    hist_f,\n",
    "    width=bar_width * 0.9,\n",
    "    alpha=0.6,\n",
    "    label='F Train',\n",
    "    color='blue'\n",
    ")\n",
    "\n",
    "# Plot the Overall System PMF\n",
    "plt.bar(\n",
    "    bin_centers,\n",
    "    hist_total,\n",
    "    width=bar_width * 0.9,\n",
    "    alpha=0.6,\n",
    "    label='Overall System',\n",
    "    color='red'\n",
    ")\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Distribution of Added Delay: F Train vs. System Average')\n",
    "plt.xlabel(f'Added Delay (seconds), Bins of {bar_width} seconds')\n",
    "plt.ylabel('Probability (PMF)')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.5)\n",
    "\n",
    "# Logarithmic scale\n",
    "plt.yscale('log')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('delay_distribution_comparison.png') # <- this is included in the repo for your viewing\n",
    "plt.close()"
   ],
   "id": "238765ed2cbcafe4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T01:39:53.317907Z",
     "start_time": "2025-12-02T01:19:04.372791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply bootstrapping on the dataset to get the KL Divergence\n",
    "TRIALS = 1000\n",
    "N_F = len(f_delays_series)\n",
    "N_TOT = len(all_delays_series)\n",
    "\n",
    "# Function to calculate KL Divergence (with LaPlace Smoothing, extra feature to prevent division by 0)\n",
    "def calculate_kl_divergence(P, Q, epsilon=1e-12):\n",
    "    # apply laplace smoothing\n",
    "    P_smoothed = (P + epsilon) / (np.sum(P) + len(P) * epsilon)\n",
    "    Q_smoothed = (Q + epsilon) / (np.sum(Q) + len(Q) * epsilon)\n",
    "\n",
    "    # calculate KL divergence\n",
    "    kl_divergence = np.sum(P_smoothed * np.log(P_smoothed / Q_smoothed))\n",
    "\n",
    "    return kl_divergence\n",
    "\n",
    "# init var\n",
    "divergences = []\n",
    "# loop 1000 times\n",
    "for i in range(TRIALS):\n",
    "    # get\n",
    "    f_sample = f_delays_series.sample(n=N_F, replace=True)\n",
    "    tot_sample = all_delays_series.sample(n=N_TOT, replace=True)\n",
    "\n",
    "    # 2. PMF GENERATION: Re-calculate the PMFs for the new samples\n",
    "    # Density=True normalizes the histogram to create the PMF array.\n",
    "    sample_f_hist, _ = np.histogram(f_sample, bins=bins, density=True)\n",
    "    sample_tot_hist, _ = np.histogram(tot_sample, bins=bins, density=True)\n",
    "\n",
    "    # 3. KL CALCULATION: Calculate the divergence\n",
    "    kl_score = calculate_kl_divergence(sample_f_hist, sample_tot_hist)\n",
    "\n",
    "    # 4. Store the result\n",
    "    divergences.append(kl_score)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Completed {i + 1} iterations.\")\n",
    "\n",
    "# convert to numpy and output mean\n",
    "kl_distribution = np.array(divergences)\n",
    "kl_mean = np.sum(kl_distribution) / TRIALS\n",
    "print(kl_mean)"
   ],
   "id": "11e8e59118fcd1d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 iterations.\n",
      "Completed 200 iterations.\n",
      "Completed 300 iterations.\n",
      "Completed 400 iterations.\n",
      "Completed 500 iterations.\n",
      "Completed 600 iterations.\n",
      "Completed 700 iterations.\n",
      "Completed 800 iterations.\n",
      "Completed 900 iterations.\n",
      "Completed 1000 iterations.\n",
      "0.03571832296035114\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:51:42.728376Z",
     "start_time": "2025-12-02T05:51:42.714856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check statistics of the sampled KL divergences\n",
    "\n",
    "# find standard deviation\n",
    "kl_std = np.std(kl_distribution)\n",
    "print(kl_std)\n",
    "\n",
    "# Find lengths of our input datas\n",
    "print(len(f_delays_series))\n",
    "print(len(all_delays_series))"
   ],
   "id": "ac834f09805e2ddd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.933949310066523e-05\n",
      "4561185\n",
      "60425425\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analysis by Time of Day",
   "id": "4c3ce7d5c4479edd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T04:14:35.850577Z",
     "start_time": "2025-12-03T04:14:35.183184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# aggregate all datasets\n",
    "f_delays_rush_series = pd.concat(f_delays_by_category['Rush Hour'], ignore_index=True)\n",
    "all_delays_rush_series = pd.concat(all_delays_by_category['Rush Hour'], ignore_index=True)\n",
    "\n",
    "f_delays_weekend_series = pd.concat(f_delays_by_category['Weekends'], ignore_index=True)\n",
    "all_delays_weekend_series = pd.concat(all_delays_by_category['Weekends'], ignore_index=True)\n",
    "\n",
    "f_delays_nights_series = pd.concat(f_delays_by_category['Nights'], ignore_index=True)\n",
    "all_delays_nights_series = pd.concat(all_delays_by_category['Nights'], ignore_index=True)\n",
    "\n",
    "# like before, define bins\n",
    "min_delay = -300\n",
    "max_delay = 1200\n",
    "bin_width = 60\n",
    "bins = np.arange(\n",
    "    start=min_delay,\n",
    "    stop=max_delay + bin_width,\n",
    "    step=bin_width\n",
    ")\n",
    "\n",
    "# calculate the PMFs for each\n",
    "hist_f_rush, _ = np.histogram(f_delays_rush_series, bins=bins, density=True)\n",
    "hist_all_rush, _ = np.histogram(all_delays_rush_series, bins=bins, density=True)\n",
    "\n",
    "hist_f_weekend, _ = np.histogram(f_delays_weekend_series, bins=bins, density=True)\n",
    "hist_all_weekend, _ = np.histogram(all_delays_weekend_series, bins=bins, density=True)\n",
    "\n",
    "hist_f_nights, _ = np.histogram(f_delays_nights_series, bins=bins, density=True)\n",
    "hist_all_nights, _ = np.histogram(all_delays_nights_series, bins=bins, density=True)\n",
    "\n",
    "print(\"Six final PMF variables have been created for KL-Divergence calculation:\")\n",
    "print(\"Rush Hour: hist_f_rush, hist_all_rush\")\n",
    "print(\"Weekends: hist_f_weekend, hist_all_weekend\")\n",
    "print(\"Nights: hist_f_nights, hist_all_nights\")"
   ],
   "id": "8dc45dc23081320",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Six final PMF variables have been created for KL-Divergence calculation:\n",
      "Rush Hour: hist_f_rush, hist_all_rush\n",
      "Weekends: hist_f_weekend, hist_all_weekend\n",
      "Nights: hist_f_nights, hist_all_nights\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T07:00:18.181105Z",
     "start_time": "2025-12-02T07:00:17.972788Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40807885170994407\n",
      "2.5933215325333667\n",
      "0.39649521594333137\n",
      "2.878245985337892\n",
      "0.373835324522228\n",
      "2.485268401585042\n"
     ]
    }
   ],
   "execution_count": 73,
   "source": [
    "# Initial analysis: look at the average delay for each. We notice that they're all pretty similar,\n",
    "# like before\n",
    "print(np.mean(f_delays_rush_series))\n",
    "print(np.mean(all_delays_rush_series))\n",
    "\n",
    "print(np.mean(f_delays_weekend_series))\n",
    "print(np.mean(all_delays_weekend_series))\n",
    "\n",
    "print(np.mean(f_delays_nights_series))\n",
    "print(np.mean(all_delays_nights_series))"
   ],
   "id": "9ca6bf0f5cee3c17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T04:21:55.796257Z",
     "start_time": "2025-12-03T04:14:38.102569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.special import rel_entr\n",
    "import numpy as np\n",
    "import pandas as pd # Needed for sampling\n",
    "\n",
    "# Helper functions\n",
    "# Calculate smoothed KL, again using Laplace smoothing\n",
    "def calculate_smoothed_kl(P, Q, epsilon=1e-12):\n",
    "    P_smoothed = (P + epsilon) / (np.sum(P) + len(P) * epsilon)\n",
    "    Q_smoothed = (Q + epsilon) / (np.sum(Q) + len(Q) * epsilon)\n",
    "\n",
    "    return np.sum(P_smoothed * np.log(P_smoothed / Q_smoothed))\n",
    "\n",
    "\n",
    "N_BOOTSTRAPS = 500  # 500 trials\n",
    "results = {'Rush Hour': [], 'Weekends': [], 'Nights': []}\n",
    "categories = ['Rush Hour', 'Weekends', 'Nights']\n",
    "\n",
    "# map names to variables\n",
    "data_map = {\n",
    "    'Rush Hour': (f_delays_rush_series, all_delays_rush_series),\n",
    "    'Weekends': (f_delays_weekend_series, all_delays_weekend_series),\n",
    "    'Nights': (f_delays_nights_series, all_delays_nights_series),\n",
    "}\n",
    "\n",
    "# begin bootstrap\n",
    "print(f\"Starting {N_BOOTSTRAPS} bootstrap iterations...\")\n",
    "\n",
    "for i in range(N_BOOTSTRAPS):\n",
    "\n",
    "    # Resample all six series in one go\n",
    "    resampled_data = {}\n",
    "    # for each category\n",
    "    for cat in categories:\n",
    "        P_series, Q_series = data_map[cat]\n",
    "\n",
    "        # get number of required samples\n",
    "        P_sample = P_series.sample(n=len(P_series), replace=True)\n",
    "        Q_sample = Q_series.sample(n=len(Q_series), replace=True)\n",
    "\n",
    "        # recalculate PMFs\n",
    "        P_hist, _ = np.histogram(P_sample, bins=bins, density=True)\n",
    "        Q_hist, _ = np.histogram(Q_sample, bins=bins, density=True)\n",
    "\n",
    "        # calculate KL and append\n",
    "        kl_score = calculate_smoothed_kl(P_hist, Q_hist)\n",
    "        results[cat].append(kl_score)\n",
    "\n",
    "print(\"Bootstrapping complete.\")\n",
    "\n",
    "# --- 4. Calculate and Print Confidence Intervals ---\n",
    "\n",
    "print(\"\\n=================================================\")\n",
    "print(\"  KL-Divergence Results\")\n",
    "print(\"=================================================\")\n",
    "\n",
    "for cat in categories:\n",
    "    scores = np.array(results[cat])\n",
    "    # The 95% CI is defined by the 2.5th and 97.5th percentiles\n",
    "    ci_low = np.percentile(scores, 2.5)\n",
    "    ci_high = np.percentile(scores, 97.5)\n",
    "\n",
    "    print(f\"KL({cat}): Mean={np.mean(scores):.5f}, Std={np.std(scores):.5f}\")"
   ],
   "id": "e353edb43ba635bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 500 bootstrap iterations...\n",
      "Bootstrapping complete.\n",
      "\n",
      "=================================================\n",
      "  KL-Divergence Results\n",
      "=================================================\n",
      "KL(Rush Hour): Mean=0.03544, Std=0.00019\n",
      "KL(Weekends): Mean=0.03620, Std=0.00027\n",
      "KL(Nights): Mean=0.03554, Std=0.00015\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analysis By Stop on the F Train",
   "id": "eab309ac97cf04ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:38:22.333975Z",
     "start_time": "2025-12-02T05:38:21.155108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze by stop\n",
    "\n",
    "# Group data into one large array\n",
    "f_stops_full = pd.concat(f_delay_by_stop, ignore_index=True)\n",
    "f_stops_full['station_id'] = f_stops_full['stop_id'].str[:-1]\n",
    "\n",
    "# Group by stop ID and calculate delay\n",
    "segment_performance = f_stops_full.groupby('station_id')['delay_added_seconds'].agg(\n",
    "    ['mean', 'count']\n",
    ").reset_index()\n",
    "\n",
    "# Sort to find worst segments\n",
    "min_count_threshold = 1000 # require at least 1,000 observations to pull out anomalies\n",
    "segment_performance_filtered = segment_performance[\n",
    "    segment_performance['count'] >= min_count_threshold\n",
    "]\n",
    "\n",
    "# Find worst bottlenecks\n",
    "worst_segments = segment_performance_filtered.sort_values(\n",
    "    by='mean',\n",
    "    ascending=False\n",
    ").head(10)\n",
    "\n",
    "# output\n",
    "print(f\"\\n--- Top 10 F-Train Bottlenecks (Segments with Highest Added Delay) ---\")\n",
    "print(worst_segments)"
   ],
   "id": "914b1c58dc57866",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 F-Train Bottlenecks (Segments with Highest Added Delay) ---\n",
      "    station_id      mean   count\n",
      "111        G09  1.803525   20257\n",
      "29         B04  1.550775   69453\n",
      "117        G15  1.444933   19313\n",
      "33         B10  0.929718   69577\n",
      "116        G14  0.842029  110286\n",
      "110        G08  0.740248  110667\n",
      "32         B08  0.733398   69947\n",
      "82         F09  0.699076   40389\n",
      "115        G13  0.670380   20196\n",
      "114        G12  0.610227   20181\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T05:47:55.813329Z",
     "start_time": "2025-12-02T05:47:55.805127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prettier Table of  Top 10 Most Delayed Stops on the F\n",
    "import numpy as np\n",
    "\n",
    "# Mappings for 10 most frequent stop IDs to their real names\n",
    "stop_id_to_name = {\n",
    "    'G09': \"67 Av (Q)\",\n",
    "    'B04': '21 St-Queensbridge (Q)',\n",
    "    'G15': '65 St (Q)',\n",
    "    'B10': '57 St (M)',\n",
    "    'G14': 'Jackson Hts-Roosevelt Av (Q)',\n",
    "    'G08': 'Forest Hills-71 Av (Q)',\n",
    "    'B08': 'Lexington Ave/63 St (Q)',\n",
    "    'F09': 'Court Sq-23 St (Q)',\n",
    "    'G13': 'Elmhurst Av (Q)',\n",
    "    'G12': 'Grand Av-Newtown (Q)',\n",
    "}\n",
    "\n",
    "# add a column that shows the real name of each stop\n",
    "segment_performance_filtered['station_name'] = segment_performance_filtered['station_id'].map(stop_id_to_name)\n",
    "\n",
    "# sort by most delayed on average\n",
    "worst_segments = segment_performance_filtered.sort_values(\n",
    "    by='mean',\n",
    "    ascending=False\n",
    ").head(10)\n",
    "\n",
    "# Create Chart\n",
    "print(f\"\\n--- Top 10 F-Train Bottlenecks (Segments with Highest Added Delay) ---\")\n",
    "# Print the stop_id, the new station_name, mean, and count\n",
    "print(worst_segments[['station_id', 'station_name', 'mean', 'count']])\n"
   ],
   "id": "5ca31baa46b6fd6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 F-Train Bottlenecks (Segments with Highest Added Delay) ---\n",
      "    station_id                  station_name      mean   count\n",
      "111        G09                     67 Av (Q)  1.803525   20257\n",
      "29         B04        21 St-Queensbridge (Q)  1.550775   69453\n",
      "117        G15                     65 St (Q)  1.444933   19313\n",
      "33         B10                     57 St (M)  0.929718   69577\n",
      "116        G14  Jackson Hts-Roosevelt Av (Q)  0.842029  110286\n",
      "110        G08        Forest Hills-71 Av (Q)  0.740248  110667\n",
      "32         B08       Lexington Ave/63 St (Q)  0.733398   69947\n",
      "82         F09            Court Sq-23 St (Q)  0.699076   40389\n",
      "115        G13               Elmhurst Av (Q)  0.670380   20196\n",
      "114        G12          Grand Av-Newtown (Q)  0.610227   20181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6k/xht3dk1x1jd5s5_8h_ljjfc40000gn/T/ipykernel_45171/978103061.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  segment_performance_filtered['station_name'] = segment_performance_filtered['station_id'].map(stop_id_to_name)\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Miscellaneous Tests and Experiments",
   "id": "8d715d1734d9dcb2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T08:37:28.316874Z",
     "start_time": "2025-12-01T08:37:28.202182Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        expected_duration_seconds  delay_added_seconds\n",
      "0                           150.0                  0.0\n",
      "1                           109.0                -19.0\n",
      "2                            79.0                 11.0\n",
      "3                            76.0                 -1.0\n",
      "4                            81.0                  0.0\n",
      "...                           ...                  ...\n",
      "155523                      213.0                 -3.0\n",
      "155524                      184.0                -73.0\n",
      "155548                      160.0                -73.0\n",
      "155572                      120.0                  0.0\n",
      "155573                       66.0                 -4.0\n",
      "\n",
      "[130929 rows x 2 columns]\n",
      "130929\n",
      "                 trip_uid stop_id  delay_added_seconds\n",
      "18888     1672576650_2..N    123N               -226.0\n",
      "41870  1672601580_2..S01R    132S               -224.0\n",
      "36097  1672594920_3..N01R    120N               -215.0\n",
      "18787  1672576590_4..S06R    235S               -200.0\n",
      "39875  1672599240_3..N01R    120N               -200.0\n",
      "...                   ...     ...                  ...\n",
      "52602  1672615920_6..N01R    610N               1910.0\n",
      "21330     1672579320_7..S    712S               3350.0\n",
      "10039  1672566930_2..S01R    207S               3824.0\n",
      "27573  1672586160_2..S01R    204S               4355.0\n",
      "15979  1672573650_2..S01R    205S               4440.0\n",
      "\n",
      "[130929 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 32,
   "source": [
    "# Test 1: Analyze One File for Functionality\n",
    "import pandas as pd\n",
    "\n",
    "# read CSV\n",
    "df = pd.read_csv(\"extracted_csv_data/subwaydatanyc_2023-01-01_stop_times.csv\")\n",
    "\n",
    "# group by trip (one train's journey), and align departure time\n",
    "df['next_actual_departure'] = df.groupby('trip_uid')['marked_past'].shift(-1)\n",
    "\n",
    "# get actual duration from stop to stop\n",
    "df['actual_duration_seconds'] = (\n",
    "    df['next_actual_departure'] - df['marked_past']\n",
    ")\n",
    "\n",
    "# get the next scheduled departure and calculate expected duration\n",
    "df['next_scheduled_departure'] = df['departure_time'].shift(-1)\n",
    "df['expected_duration_seconds'] = (\n",
    "    df['next_scheduled_departure'] - df['departure_time']\n",
    ")\n",
    "\n",
    "# filter out for trips that are longer than 30 seconds and are relatively close to expected trip time\n",
    "df_filtered = df[df['actual_duration_seconds'] > (df['expected_duration_seconds'] * 0.5)]\n",
    "df_filtered = df_filtered[df_filtered['actual_duration_seconds'] > 30]\n",
    "\n",
    "# calculate the delay added/stop\n",
    "df_filtered['delay_added_seconds'] = (\n",
    "    df_filtered['actual_duration_seconds'] - df_filtered['expected_duration_seconds']\n",
    ")\n",
    "\n",
    "# remove any stops with NaN (last stop of each trip)\n",
    "df_final = df_filtered.dropna(subset=['delay_added_seconds'])"
   ],
   "id": "fa2769ce52f8cb93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T08:40:42.111436Z",
     "start_time": "2025-12-01T08:40:42.023427Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.006378189094547274\n",
      "5.210709504685409\n",
      "1.9174973488865323\n",
      "2.0375928938585033\n"
     ]
    }
   ],
   "execution_count": 34,
   "source": [
    "# Test 2: Miscellaneous Analysis on Train Data (organized by route)\n",
    "\n",
    "total_average = df_final['delay_added_seconds'].mean()\n",
    "\n",
    "df_subway_only = df_final[~df_final['stop_id'].str.startswith('S')].copy()\n",
    "df_f_train = df_subway_only[\n",
    "    df_subway_only['trip_uid'].astype(str).str.contains('F')\n",
    "].copy()\n",
    "\n",
    "two_train_df = df_final[\n",
    "    df_final['trip_uid'].str.contains(r'_2\\.\\.', regex=True)\n",
    "].copy()\n",
    "\n",
    "six_train_df = df_final[\n",
    "    df_final['trip_uid'].str.contains(r'_6\\.\\.', regex=True)\n",
    "].copy()\n",
    "\n",
    "two_avg = two_train_df['delay_added_seconds'].mean()\n",
    "six_avg = six_train_df['delay_added_seconds'].mean()\n",
    "F_average = df_f_train['delay_added_seconds'].mean()\n",
    "\n",
    "print(F_average)\n",
    "print(two_avg)\n",
    "print(six_avg)\n",
    "print(total_average)"
   ],
   "id": "a345e80844586531"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
